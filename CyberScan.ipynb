{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-07T18:06:33.808501Z",
     "iopub.status.busy": "2024-11-07T18:06:33.807713Z",
     "iopub.status.idle": "2024-11-07T18:06:35.187246Z",
     "shell.execute_reply": "2024-11-07T18:06:35.186270Z",
     "shell.execute_reply.started": "2024-11-07T18:06:33.808449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T18:10:32.034850Z",
     "iopub.status.busy": "2024-11-07T18:10:32.033890Z",
     "iopub.status.idle": "2024-11-07T18:10:55.605257Z",
     "shell.execute_reply": "2024-11-07T18:10:55.604069Z",
     "shell.execute_reply.started": "2024-11-07T18:10:32.034805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community faiss-cpu transformers pypdf langchain_groq\n",
    "!pip install sentence_transformers\n",
    "!pip install groq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API KEYS\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T18:10:55.608050Z",
     "iopub.status.busy": "2024-11-07T18:10:55.607608Z",
     "iopub.status.idle": "2024-11-07T18:10:55.968165Z",
     "shell.execute_reply": "2024-11-07T18:10:55.967199Z",
     "shell.execute_reply.started": "2024-11-07T18:10:55.607999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "gemma= user_secrets.get_secret(\"Gemma\")\n",
    "groq = user_secrets.get_secret(\"Groq\")\n",
    "huggingface = user_secrets.get_secret(\"HuggingFace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T18:10:55.969647Z",
     "iopub.status.busy": "2024-11-07T18:10:55.969348Z",
     "iopub.status.idle": "2024-11-07T18:10:55.973842Z",
     "shell.execute_reply": "2024-11-07T18:10:55.972886Z",
     "shell.execute_reply.started": "2024-11-07T18:10:55.969615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF Vectorization\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T18:10:55.977053Z",
     "iopub.status.busy": "2024-11-07T18:10:55.976650Z",
     "iopub.status.idle": "2024-11-07T18:10:55.991081Z",
     "shell.execute_reply": "2024-11-07T18:10:55.990176Z",
     "shell.execute_reply.started": "2024-11-07T18:10:55.977012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def load_documents(file_path):\n",
    "    \"\"\"Load PDF documents using PyPDFLoader.\"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load()\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks using RecursiveCharacterTextSplitter.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def create_embeddings():\n",
    "    \"\"\"Create HuggingFace embeddings.\"\"\"\n",
    "    return HuggingFaceBgeEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-l6-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "\n",
    "def create_vectorstore(final_documents, embeddings):\n",
    "    \"\"\"Create a FAISS vector store from the documents and embeddings.\"\"\"\n",
    "    return FAISS.from_documents(final_documents, embeddings)\n",
    "\n",
    "def setup_llm():\n",
    "    \"\"\"Set up the Hugging Face model.\"\"\"\n",
    "    return HuggingFaceHub(\n",
    "#         repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        model_kwargs={\"temperature\": 0.5, \"max_length\": 1000}\n",
    "    )\n",
    "\n",
    "def create_prompt_template():\n",
    "    \"\"\"Create a prompt template for the RetrievalQA.\"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "    \"You are a knowledgeable assistant specialized in extracting information from PDF documents.\"\n",
    "    \"When answering questions, you must base your responses solely on the content of the PDF provided.\"\n",
    "    \"Do not include any information or context outside of this document.\"\n",
    "    \n",
    "    context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    return PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "def setup_retrieval_qa(llm, retriever, prompt):\n",
    "    \"\"\"Set up the RetrievalQA chain.\"\"\"\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "\n",
    "def main(file_path):\n",
    "    \"\"\"Main function to execute the document processing and retrieval setup.\"\"\"\n",
    "    documents = load_documents(file_path)\n",
    "    final_documents = split_documents(documents)\n",
    "    embeddings = create_embeddings()\n",
    "    vectorstore = create_vectorstore(final_documents, embeddings)\n",
    "    \n",
    "    # Create a retriever from the vector store\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    llm = setup_llm()\n",
    "    prompt = create_prompt_template()\n",
    "    retrieval_qa = setup_retrieval_qa(llm, retriever, prompt)\n",
    "    \n",
    "    return retrieval_qa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Dump/Load\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T18:10:55.992564Z",
     "iopub.status.busy": "2024-11-07T18:10:55.992204Z",
     "iopub.status.idle": "2024-11-07T18:10:56.004754Z",
     "shell.execute_reply": "2024-11-07T18:10:56.003850Z",
     "shell.execute_reply.started": "2024-11-07T18:10:55.992521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"retrieval_qa_Model.pkl\", 'wb') as f:\n",
    "#     pickle.dump(retrieval_qa, f)\n",
    "# print(f\"Model saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T18:10:56.006105Z",
     "iopub.status.busy": "2024-11-07T18:10:56.005779Z",
     "iopub.status.idle": "2024-11-07T18:10:56.016685Z",
     "shell.execute_reply": "2024-11-07T18:10:56.015963Z",
     "shell.execute_reply.started": "2024-11-07T18:10:56.006074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# with open(\"retrieval_qa_Model.pkl\", 'rb') as f:\n",
    "#     loaded_retrieval_qa = pickle.load(f)\n",
    "\n",
    "# print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:27.097627Z",
     "iopub.status.busy": "2024-11-07T19:53:27.097223Z",
     "iopub.status.idle": "2024-11-07T19:53:29.036540Z",
     "shell.execute_reply": "2024-11-07T19:53:29.035560Z",
     "shell.execute_reply.started": "2024-11-07T19:53:27.097591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# file_path = \"/content/drive/MyDrive/Colab Notebooks/CyberScan_Datahack/Cybersecurity Report Template.pdf\"\n",
    "file_path = \"/kaggle/input/report/Cybersecurity Report Overview.pdf\"\n",
    "# file_path=\"/kaggle/input/report/Cybersecurity Report Template.pdf\"\n",
    "retrieval_qa = main(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:29.039324Z",
     "iopub.status.busy": "2024-11-07T19:53:29.038468Z",
     "iopub.status.idle": "2024-11-07T19:53:29.043345Z",
     "shell.execute_reply": "2024-11-07T19:53:29.042452Z",
     "shell.execute_reply.started": "2024-11-07T19:53:29.039277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# result=retrieval_qa.invoke({\"query\":\"What measures are in place to ensure the confidentiality, integrity, and availability of data?\"})\n",
    "# result=result['result']\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diplaying the code\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:29.044818Z",
     "iopub.status.busy": "2024-11-07T19:53:29.044424Z",
     "iopub.status.idle": "2024-11-07T19:53:29.057197Z",
     "shell.execute_reply": "2024-11-07T19:53:29.056310Z",
     "shell.execute_reply.started": "2024-11-07T19:53:29.044770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def arrange(res):\n",
    "  if \"Answer:\" in res:\n",
    "      answer = res.split(\"Answer:\")[1].strip()\n",
    "  elif \"answer:\" in res:\n",
    "      answer = res.split(\"answer:\")[1].strip()\n",
    "  else:\n",
    "      # Fallback to the entire result if no split pattern is found\n",
    "      answer = res\n",
    "  return answer\n",
    "\n",
    "def print_with_newline_on_period(text):\n",
    "    modified_text = re.sub(r'\\.', '.\\n', text)\n",
    "    return modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:29.059457Z",
     "iopub.status.busy": "2024-11-07T19:53:29.059148Z",
     "iopub.status.idle": "2024-11-07T19:53:44.575322Z",
     "shell.execute_reply": "2024-11-07T19:53:44.574515Z",
     "shell.execute_reply.started": "2024-11-07T19:53:29.059423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file For Question_Answer\n",
    "with open('/kaggle/input/knowledge/ques_ans.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "question = []\n",
    "question_domain={}\n",
    "answer = []\n",
    "domain=[]\n",
    "# Access each domain's question and answer\n",
    "for domain_name, details in data.items():\n",
    "    question.append(details['question'])\n",
    "    question_domain[domain_name]=details['question']\n",
    "\n",
    "\n",
    "\n",
    "    answer.append(print_with_newline_on_period(details['answer']))\n",
    "    domain.append(domain_name)\n",
    "#     print(f\"Domain: {domain_name}\")\n",
    "#     print(f\"Question: {question}\")\n",
    "#     print(f\"Answer: {answer}\\n\")\n",
    "\n",
    "\n",
    "answer_predicted=[]\n",
    "for i in range(len(question)):\n",
    "    query=question[i]\n",
    "    result=retrieval_qa.invoke({\"query\":query})\n",
    "    result=result['result']\n",
    "    result=arrange(result)\n",
    "    answer_predicted.append(result)\n",
    "\n",
    "\n",
    "  # print(\"-\"*50)\n",
    "  # print(f\"Domain: {domain[i]}\")\n",
    "  # print(\"-\"*50)\n",
    "\n",
    "  # print(f\"Question: {query}\")\n",
    "  # print(\"-\"*50)\n",
    "  # print(f\"Retrieved_Answer: {result}\\n\")\n",
    "  # print(\"-\"*50)\n",
    "\n",
    "  # print(f\"Acutal_Req: {answer[i]}\\n\")\n",
    "  # print(\"-\"*50)\n",
    "  # print(\"-\"*50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain wise Scoring\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:44.576807Z",
     "iopub.status.busy": "2024-11-07T19:53:44.576491Z",
     "iopub.status.idle": "2024-11-07T19:53:44.624756Z",
     "shell.execute_reply": "2024-11-07T19:53:44.623922Z",
     "shell.execute_reply.started": "2024-11-07T19:53:44.576774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate similarity between actual response and predicted answer with context\n",
    "def evaluate_domain_response(question, actual_answer, predicted_answer):\n",
    "    \"\"\"Evaluate the risk score based on the similarity between actual and predicted answers.\"\"\"\n",
    "    # Combine question with answers\n",
    "    actual_combined = f\"{question} {actual_answer}\"\n",
    "    predicted_combined = f\"{question} {predicted_answer}\"\n",
    "    \n",
    "    # Vectorize and calculate similarity\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([actual_combined, predicted_combined])\n",
    "    similarity = cosine_similarity(vectors[0], vectors[1])\n",
    "    \n",
    "    # Interpret similarity into a risk score (0 - high risk, 1 - low risk)\n",
    "    risk_score = 1 - similarity[0][0]  # Inverse of similarity for risk (0 is high risk, 1 is low risk)\n",
    "    \n",
    "    return risk_score, similarity[0][0]  # Return risk score and similarity score\n",
    "\n",
    "# Evaluate responses for each domain\n",
    "domain_scores = {}\n",
    "for i in range(len(question)):\n",
    "    # Calculate score with context\n",
    "    risk_score, similarity_score = evaluate_domain_response(question[i], answer,answer_predicted[i])\n",
    "    \n",
    "    # Store the risk score in the domain_scores dictionary\n",
    "    domain_scores[domain[i]] = {\n",
    "        'risk_score': risk_score,\n",
    "        'similarity_score': similarity_score\n",
    "    }\n",
    "\n",
    "# # Identify the weakest domain (highest risk)\n",
    "# weakest_domain = min(domain_scores, key=lambda x: domain_scores[x]['risk_score'])\n",
    "# print(f\"Domain with highest risk: {weakest_domain}\")\n",
    "# print(f\"Scores by domain: {domain_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:44.626224Z",
     "iopub.status.busy": "2024-11-07T19:53:44.625845Z",
     "iopub.status.idle": "2024-11-07T19:53:44.633498Z",
     "shell.execute_reply": "2024-11-07T19:53:44.632592Z",
     "shell.execute_reply.started": "2024-11-07T19:53:44.626178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_domain_scores(domain_scores):\n",
    "    for domain, scores in domain_scores.items():\n",
    "        risk_score = scores['risk_score']\n",
    "        similarity_score = scores['similarity_score']\n",
    "        print(\"-\"*50)\n",
    "        print(f\"Domain: {domain}\")\n",
    "        print(\"-\"*50)\n",
    "        print(f\" Risk Score: {risk_score:.2f}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "\n",
    "# Identify the weakest domain (highest risk)\n",
    "weakest_domain = min(domain_scores, key=lambda x: domain_scores[x]['risk_score'])\n",
    "print(f\"Domain with highest risk: {weakest_domain}\")\n",
    "\n",
    "\n",
    "sorted_domains = sorted(domain_scores, key=lambda x: domain_scores[x]['risk_score'])\n",
    "weakest_4_domains = sorted_domains[:4]\n",
    "print(f\"Domain with highest risk: {weakest_4_domains}\")\n",
    "\n",
    "# Print scores for each domain\n",
    "# print_domain_scores(domain_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:44.637011Z",
     "iopub.status.busy": "2024-11-07T19:53:44.636365Z",
     "iopub.status.idle": "2024-11-07T19:53:44.648592Z",
     "shell.execute_reply": "2024-11-07T19:53:44.647705Z",
     "shell.execute_reply.started": "2024-11-07T19:53:44.636967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def print_domain_scores(domain_scores):\n",
    "    # Create a PrettyTable object\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Domain\", \"Risk Score\"]\n",
    "\n",
    "    # Populate the table with domain scores\n",
    "    for domain, scores in domain_scores.items():\n",
    "        risk_score = scores['risk_score']\n",
    "        similarity_score = scores['similarity_score']\n",
    "        table.add_row([domain, f\"{risk_score:.2f}\" ])\n",
    "\n",
    "    # Print the table\n",
    "    print(table)\n",
    "print_domain_scores(domain_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groq_Follow_up\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:44.649953Z",
     "iopub.status.busy": "2024-11-07T19:53:44.649651Z",
     "iopub.status.idle": "2024-11-07T19:53:45.194597Z",
     "shell.execute_reply": "2024-11-07T19:53:45.193863Z",
     "shell.execute_reply.started": "2024-11-07T19:53:44.649896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Set up your Groq client with the correct API key\n",
    "client = Groq(\n",
    "    api_key=groq,  # Replace 'groq' with your actual API key if it's not defined\n",
    ")\n",
    "\n",
    "# Define the system prompt and the user's question\n",
    "system_prompt =\"\"\" You are a highly intelligent assistant capable of generating insightful follow-up questions based on an initial input. Given a question and a specific domain, your task is to generate 10 follow-up questions that are directly related to the domain and provide a deeper or more detailed exploration of the subject within that domain.\n",
    "\n",
    "The follow-up questions should:\n",
    "\n",
    "1. Be highly relevant to the original question, exploring various aspects within the domain.\n",
    "2. Encourage further clarification, elaboration, or exploration of the domain.\n",
    "3. Provide valuable insights that help the user gain a deeper understanding of the domain in question.\n",
    "\n",
    "Input Question: \"{user_input}\"\n",
    "Domain: \"{user_domain}\"\n",
    "\n",
    "Generate 10 thoughtful follow-up questions that delve into the topic of the original question, specifically focused on the domain of \"{user_domain}\":\n",
    "\"\"\"\n",
    "# User's input question\n",
    "user_input = \"Explain the importance of fast language models\"\n",
    "user_domain=\"Network Security\"\n",
    "# Make the API call with system and user messages\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},  # System message setting the behavior\n",
    "        {\"role\": \"user\", \"content\": user_input},  # User's input\n",
    "        {\"role\": \"user\", \"content\": user_domain}  # User's input\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",  # Specify the model you're using\n",
    ")\n",
    "\n",
    "output_text = chat_completion.choices[0].message.content\n",
    "import re\n",
    "follow_up_questions = re.findall(r'\\d+\\.\\s([A-Za-z0-9\\s\\(\\),\\?\\'\"-]+[?])', output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:45.195974Z",
     "iopub.status.busy": "2024-11-07T19:53:45.195661Z",
     "iopub.status.idle": "2024-11-07T19:53:46.552593Z",
     "shell.execute_reply": "2024-11-07T19:53:46.551668Z",
     "shell.execute_reply.started": "2024-11-07T19:53:45.195941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_sentence_embedding(text, model, tokenizer):\n",
    "    # Tokenize the input text and get the attention mask\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the hidden states and take the average to get the sentence embedding\n",
    "    hidden_states = outputs.last_hidden_state  # Shape: (1, sequence_length, hidden_dim)\n",
    "    sentence_embedding = hidden_states.mean(dim=1)  # Shape: (1, hidden_dim)\n",
    "    \n",
    "    return sentence_embedding\n",
    "\n",
    "# Save the model and tokenizer\n",
    "save_directory = './bert_model'\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(\"Model and tokenizer saved successfully.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:46.554254Z",
     "iopub.status.busy": "2024-11-07T19:53:46.553845Z",
     "iopub.status.idle": "2024-11-07T19:53:46.702850Z",
     "shell.execute_reply": "2024-11-07T19:53:46.701950Z",
     "shell.execute_reply.started": "2024-11-07T19:53:46.554210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load the model and tokenizer from the saved directory\n",
    "load_directory = './bert_model'\n",
    "model = BertModel.from_pretrained(load_directory)\n",
    "tokenizer = BertTokenizer.from_pretrained(load_directory)\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:46.704235Z",
     "iopub.status.busy": "2024-11-07T19:53:46.703942Z",
     "iopub.status.idle": "2024-11-07T19:53:49.847774Z",
     "shell.execute_reply": "2024-11-07T19:53:49.846709Z",
     "shell.execute_reply.started": "2024-11-07T19:53:46.704204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    response =question_domain[weakest_4_domains[i]]\n",
    "    # Get the embedding for the response\n",
    "    response_embedding = get_sentence_embedding(response, model, tokenizer)\n",
    "    # Get embeddings for each question\n",
    "    question_embeddings = [get_sentence_embedding(question, model, tokenizer) for question in follow_up_questions]\n",
    "    \n",
    "    cosine_similarities = []\n",
    "    for question_embedding in question_embeddings:\n",
    "        similarity = cosine_similarity(response_embedding, question_embedding)\n",
    "        cosine_similarities.append(similarity[0][0])\n",
    "    \n",
    "    most_relevant_question_index = np.argmax(cosine_similarities)\n",
    "    most_relevant_question = questions[most_relevant_question_index]\n",
    "    print(\"Domain:\",weakest_4_domains[i])\n",
    "    print(\"Response:\", response)\n",
    "    print(\"\\nMost relevant question:\", most_relevant_question)\n",
    "    print(\"Cosine similarity score:\", cosine_similarities[most_relevant_question_index])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:49.849834Z",
     "iopub.status.busy": "2024-11-07T19:53:49.849203Z",
     "iopub.status.idle": "2024-11-07T19:53:49.853880Z",
     "shell.execute_reply": "2024-11-07T19:53:49.852913Z",
     "shell.execute_reply.started": "2024-11-07T19:53:49.849787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between response and each question\n",
    "\n",
    "# Print results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T19:53:49.855992Z",
     "iopub.status.busy": "2024-11-07T19:53:49.855523Z",
     "iopub.status.idle": "2024-11-07T19:53:53.063279Z",
     "shell.execute_reply": "2024-11-07T19:53:53.062233Z",
     "shell.execute_reply.started": "2024-11-07T19:53:49.855948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_most_relevant_question(weakest_4_domains, question_domain, follow_up_questions, questions, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to find the most relevant follow-up question based on cosine similarity with the response.\n",
    "\n",
    "    Parameters:\n",
    "    - weakest_4_domains: List of the weakest 4 domains.\n",
    "    - question_domain: Dictionary with domain names as keys and their corresponding responses as values.\n",
    "    - follow_up_questions: List of follow-up questions to compare.\n",
    "    - questions: List of all possible questions to choose the most relevant one.\n",
    "    - model: Pre-trained BERT model.\n",
    "    - tokenizer: Pre-trained BERT tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    - A list of tuples containing the domain, response, most relevant question, and cosine similarity score.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i in range(4):\n",
    "        response = question_domain[weakest_4_domains[i]]\n",
    "        # Get the embedding for the response\n",
    "        response_embedding = get_sentence_embedding(response, model, tokenizer)\n",
    "        \n",
    "        # Get embeddings for each question\n",
    "        question_embeddings = [get_sentence_embedding(question, model, tokenizer) for question in follow_up_questions]\n",
    "        \n",
    "        cosine_similarities = []\n",
    "        for question_embedding in question_embeddings:\n",
    "            similarity = cosine_similarity(response_embedding, question_embedding)\n",
    "            cosine_similarities.append(similarity[0][0])\n",
    "        \n",
    "        # Find the most relevant question based on cosine similarity\n",
    "        most_relevant_question_index = np.argmax(cosine_similarities)\n",
    "        most_relevant_question = questions[most_relevant_question_index]\n",
    "\n",
    "        # Store the result for this domain\n",
    "        results.append((\n",
    "            weakest_4_domains[i],\n",
    "            response,\n",
    "            most_relevant_question,\n",
    "            cosine_similarities[most_relevant_question_index]\n",
    "        ))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "results = get_most_relevant_question(\n",
    "    weakest_4_domains, \n",
    "    question_domain, \n",
    "    follow_up_questions, \n",
    "    questions, \n",
    "    model, \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(\"Domain:\", result[0])\n",
    "    print(\"Response:\", result[1])\n",
    "    print(\"\\nMost relevant question:\", result[2])\n",
    "    print(\"Cosine similarity score:\", result[3])\n",
    "    print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6023364,
     "sourceId": 9822953,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6024070,
     "sourceId": 9823883,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
